{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%run data_package_loading.py # Code loads data as well as packages that are relevant across most project phases\n",
    "%matplotlib inline\n",
    "\n",
    "uci_features = ['28',  '48',  '64', '105', '128', '153', '241', '281', '318', '336', \n",
    "                '338', '378', '433', '442', '451', '453', '455', '472', '475', '493']\n",
    "\n",
    "madelon_features = ['feat_257', 'feat_269', 'feat_308', 'feat_315', 'feat_336',\n",
    "                   'feat_341', 'feat_395', 'feat_504', 'feat_526', 'feat_639',\n",
    "                   'feat_681', 'feat_701', 'feat_724', 'feat_736', 'feat_769',\n",
    "                   'feat_808', 'feat_829', 'feat_867', 'feat_920', 'feat_956']\n",
    "\n",
    "Xuci_1 = Xuci_1[uci_features]\n",
    "Xuci_2 = Xuci_2[uci_features]\n",
    "Xuci_3 = Xuci_3[uci_features]\n",
    "\n",
    "# !conda install -y psycopg2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel, RFECV \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "\n",
    "Xdb_1 = pd.read_pickle('data/madelon_db_1')\n",
    "Xdb_2 = pd.read_pickle('data/madelon_db_2')\n",
    "Xdb_3 = pd.read_pickle('data/madelon_db_3')\n",
    "\n",
    "\n",
    "ydb_1 = Xdb_1['target']\n",
    "ydb_2 = Xdb_2['target']\n",
    "ydb_3 = Xdb_3['target']\n",
    "Xdb_1 = Xdb_1[madelon_features]\n",
    "Xdb_2 = Xdb_2[madelon_features]\n",
    "Xdb_3 = Xdb_3[madelon_features]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force of testing Feature Correlations. \n",
    "\n",
    "This approach is based on two pieces of information:\n",
    "1. Previous grid searches of PCA consistently found that 5 PCA components provided the best fit. This suggests that the Database data includes five true predictors.\n",
    "2. the true predictors are independent from each other.\n",
    "\n",
    "The following code will attempt to find the set of 5 feature with the least amount of correlations. This will be accomplished by \n",
    "1. Testing every set of 5 features from the 20 features previously identified.\n",
    "1. Creating a cross correlation matrix for each set of 5\n",
    "1. Taking the sum of the absolute values of the correlation matrix.\n",
    "\n",
    "The assumption is that the set of 5 features with the lowest total correlation will be the most independent set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23588e99294443fbf922a3170ed2917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15504), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corr_results = []\n",
    "combos = list(itertools.combinations(Xdb_1, 5))\n",
    "\n",
    "for cols in tqdm_notebook(combos):\n",
    "    corr1 = Xdb_1[list(cols)].corr()\n",
    "    corr2 = Xdb_2[list(cols)].corr()\n",
    "    corr3 = Xdb_3[list(cols)].corr()\n",
    "\n",
    "    tmp = pd.concat([corr1, corr2, corr3])\n",
    "    mean_corr = abs(tmp).groupby(tmp.index).mean()\n",
    "\n",
    "\n",
    "    corr_results.append({'columns': cols,\n",
    "                         'Xdb_1_corr_sum': abs(corr1).sum().sum() - 5,\n",
    "                         'Xdb_2_corr_sum': abs(corr2).sum().sum() - 5,\n",
    "                         'Xdb_3_corr_sum': abs(corr3).sum().sum() - 5,\n",
    "                         'mean_corr_sum': mean_corr.sum().sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xdb_1_corr_sum</th>\n",
       "      <th>Xdb_2_corr_sum</th>\n",
       "      <th>Xdb_3_corr_sum</th>\n",
       "      <th>columns</th>\n",
       "      <th>mean_corr_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>5.875845</td>\n",
       "      <td>5.872196</td>\n",
       "      <td>5.888319</td>\n",
       "      <td>(feat_257, feat_526, feat_681, feat_736, feat_...</td>\n",
       "      <td>5.878787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>7.691497</td>\n",
       "      <td>7.655846</td>\n",
       "      <td>7.592507</td>\n",
       "      <td>(feat_257, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>7.646617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14895</th>\n",
       "      <td>7.637696</td>\n",
       "      <td>7.655231</td>\n",
       "      <td>7.667335</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>7.653421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15322</th>\n",
       "      <td>7.832736</td>\n",
       "      <td>7.839899</td>\n",
       "      <td>7.783426</td>\n",
       "      <td>(feat_681, feat_724, feat_736, feat_920, feat_...</td>\n",
       "      <td>7.818687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>7.837366</td>\n",
       "      <td>7.894866</td>\n",
       "      <td>7.896389</td>\n",
       "      <td>(feat_257, feat_504, feat_526, feat_736, feat_...</td>\n",
       "      <td>7.876207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14888</th>\n",
       "      <td>7.899120</td>\n",
       "      <td>7.893090</td>\n",
       "      <td>7.891945</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_808, feat_...</td>\n",
       "      <td>7.894719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718</th>\n",
       "      <td>7.948210</td>\n",
       "      <td>7.918848</td>\n",
       "      <td>7.854657</td>\n",
       "      <td>(feat_257, feat_681, feat_736, feat_769, feat_...</td>\n",
       "      <td>7.907238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>7.923472</td>\n",
       "      <td>7.955926</td>\n",
       "      <td>7.884594</td>\n",
       "      <td>(feat_257, feat_504, feat_736, feat_769, feat_...</td>\n",
       "      <td>7.921331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>7.954591</td>\n",
       "      <td>7.909024</td>\n",
       "      <td>7.965330</td>\n",
       "      <td>(feat_257, feat_336, feat_526, feat_681, feat_...</td>\n",
       "      <td>7.942982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>8.039368</td>\n",
       "      <td>7.934679</td>\n",
       "      <td>7.966066</td>\n",
       "      <td>(feat_257, feat_308, feat_504, feat_701, feat_...</td>\n",
       "      <td>7.980038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>8.049228</td>\n",
       "      <td>8.019662</td>\n",
       "      <td>7.940713</td>\n",
       "      <td>(feat_257, feat_639, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.003201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>8.070623</td>\n",
       "      <td>7.973879</td>\n",
       "      <td>7.979041</td>\n",
       "      <td>(feat_257, feat_308, feat_526, feat_681, feat_...</td>\n",
       "      <td>8.007848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>8.082042</td>\n",
       "      <td>8.074152</td>\n",
       "      <td>7.995866</td>\n",
       "      <td>(feat_257, feat_504, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.050687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12070</th>\n",
       "      <td>8.041623</td>\n",
       "      <td>8.053898</td>\n",
       "      <td>8.090013</td>\n",
       "      <td>(feat_336, feat_526, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.061845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>8.108501</td>\n",
       "      <td>8.141844</td>\n",
       "      <td>8.032311</td>\n",
       "      <td>(feat_308, feat_395, feat_681, feat_701, feat_...</td>\n",
       "      <td>8.094219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>8.114682</td>\n",
       "      <td>8.103782</td>\n",
       "      <td>8.081794</td>\n",
       "      <td>(feat_257, feat_395, feat_526, feat_681, feat_...</td>\n",
       "      <td>8.100086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9226</th>\n",
       "      <td>8.129768</td>\n",
       "      <td>8.088761</td>\n",
       "      <td>8.097298</td>\n",
       "      <td>(feat_308, feat_701, feat_769, feat_808, feat_...</td>\n",
       "      <td>8.105276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>8.146899</td>\n",
       "      <td>8.108159</td>\n",
       "      <td>8.077372</td>\n",
       "      <td>(feat_257, feat_341, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.110810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070</th>\n",
       "      <td>8.210892</td>\n",
       "      <td>8.236887</td>\n",
       "      <td>8.140580</td>\n",
       "      <td>(feat_395, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>8.196120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>8.239205</td>\n",
       "      <td>8.167829</td>\n",
       "      <td>8.206500</td>\n",
       "      <td>(feat_308, feat_504, feat_701, feat_769, feat_...</td>\n",
       "      <td>8.204512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>8.252071</td>\n",
       "      <td>8.253272</td>\n",
       "      <td>8.139390</td>\n",
       "      <td>(feat_257, feat_395, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.214911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>8.217414</td>\n",
       "      <td>8.268188</td>\n",
       "      <td>8.278337</td>\n",
       "      <td>(feat_257, feat_526, feat_736, feat_769, feat_...</td>\n",
       "      <td>8.254646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>8.241858</td>\n",
       "      <td>8.273622</td>\n",
       "      <td>8.268059</td>\n",
       "      <td>(feat_526, feat_639, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.261180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>8.348064</td>\n",
       "      <td>8.254235</td>\n",
       "      <td>8.192323</td>\n",
       "      <td>(feat_257, feat_308, feat_504, feat_769, feat_...</td>\n",
       "      <td>8.264874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14891</th>\n",
       "      <td>8.292666</td>\n",
       "      <td>8.331288</td>\n",
       "      <td>8.303295</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_829, feat_...</td>\n",
       "      <td>8.309083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15358</th>\n",
       "      <td>8.326981</td>\n",
       "      <td>8.333203</td>\n",
       "      <td>8.281753</td>\n",
       "      <td>(feat_681, feat_736, feat_808, feat_920, feat_...</td>\n",
       "      <td>8.313979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>8.316888</td>\n",
       "      <td>8.282006</td>\n",
       "      <td>8.344493</td>\n",
       "      <td>(feat_257, feat_341, feat_526, feat_681, feat_...</td>\n",
       "      <td>8.314463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14864</th>\n",
       "      <td>8.297316</td>\n",
       "      <td>8.324891</td>\n",
       "      <td>8.335630</td>\n",
       "      <td>(feat_526, feat_681, feat_724, feat_736, feat_...</td>\n",
       "      <td>8.319279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>8.413909</td>\n",
       "      <td>8.330627</td>\n",
       "      <td>8.272169</td>\n",
       "      <td>(feat_257, feat_336, feat_681, feat_736, feat_...</td>\n",
       "      <td>8.338902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7262</th>\n",
       "      <td>8.381765</td>\n",
       "      <td>8.347559</td>\n",
       "      <td>8.369122</td>\n",
       "      <td>(feat_308, feat_315, feat_504, feat_769, feat_...</td>\n",
       "      <td>8.366148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>18.167595</td>\n",
       "      <td>18.230791</td>\n",
       "      <td>18.201692</td>\n",
       "      <td>(feat_269, feat_341, feat_395, feat_724, feat_...</td>\n",
       "      <td>18.200026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>18.204316</td>\n",
       "      <td>18.175931</td>\n",
       "      <td>18.228693</td>\n",
       "      <td>(feat_315, feat_336, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.202980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>18.222520</td>\n",
       "      <td>18.217978</td>\n",
       "      <td>18.232539</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>18.224345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>18.235253</td>\n",
       "      <td>18.207738</td>\n",
       "      <td>18.321367</td>\n",
       "      <td>(feat_315, feat_526, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.254786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10684</th>\n",
       "      <td>18.239700</td>\n",
       "      <td>18.219710</td>\n",
       "      <td>18.340381</td>\n",
       "      <td>(feat_315, feat_526, feat_639, feat_867, feat_...</td>\n",
       "      <td>18.266597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>18.306723</td>\n",
       "      <td>18.320649</td>\n",
       "      <td>18.305784</td>\n",
       "      <td>(feat_257, feat_639, feat_829, feat_867, feat_...</td>\n",
       "      <td>18.311052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11882</th>\n",
       "      <td>18.316137</td>\n",
       "      <td>18.343520</td>\n",
       "      <td>18.328314</td>\n",
       "      <td>(feat_336, feat_504, feat_639, feat_829, feat_...</td>\n",
       "      <td>18.329324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>18.332220</td>\n",
       "      <td>18.308996</td>\n",
       "      <td>18.349334</td>\n",
       "      <td>(feat_257, feat_315, feat_336, feat_639, feat_...</td>\n",
       "      <td>18.330184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>18.441188</td>\n",
       "      <td>18.370424</td>\n",
       "      <td>18.414225</td>\n",
       "      <td>(feat_269, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>18.408612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12883</th>\n",
       "      <td>18.401539</td>\n",
       "      <td>18.425342</td>\n",
       "      <td>18.413806</td>\n",
       "      <td>(feat_341, feat_504, feat_639, feat_829, feat_...</td>\n",
       "      <td>18.413563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>18.461992</td>\n",
       "      <td>18.533534</td>\n",
       "      <td>18.607556</td>\n",
       "      <td>(feat_336, feat_341, feat_526, feat_639, feat_...</td>\n",
       "      <td>18.534361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>18.516303</td>\n",
       "      <td>18.539757</td>\n",
       "      <td>18.561910</td>\n",
       "      <td>(feat_341, feat_395, feat_526, feat_724, feat_...</td>\n",
       "      <td>18.539323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>18.587545</td>\n",
       "      <td>18.515597</td>\n",
       "      <td>18.520000</td>\n",
       "      <td>(feat_269, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.541048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>18.599210</td>\n",
       "      <td>18.538693</td>\n",
       "      <td>18.587681</td>\n",
       "      <td>(feat_269, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>18.575194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>18.696703</td>\n",
       "      <td>18.708055</td>\n",
       "      <td>18.701262</td>\n",
       "      <td>(feat_257, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.702007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11378</th>\n",
       "      <td>18.665386</td>\n",
       "      <td>18.695446</td>\n",
       "      <td>18.753856</td>\n",
       "      <td>(feat_336, feat_341, feat_639, feat_867, feat_...</td>\n",
       "      <td>18.704896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>18.747550</td>\n",
       "      <td>18.706990</td>\n",
       "      <td>18.819577</td>\n",
       "      <td>(feat_315, feat_526, feat_639, feat_701, feat_...</td>\n",
       "      <td>18.758039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>18.758017</td>\n",
       "      <td>18.765334</td>\n",
       "      <td>18.772074</td>\n",
       "      <td>(feat_257, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.765142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>18.815842</td>\n",
       "      <td>18.759403</td>\n",
       "      <td>18.781236</td>\n",
       "      <td>(feat_269, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.785494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>18.837957</td>\n",
       "      <td>18.843159</td>\n",
       "      <td>18.877827</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>18.852981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>18.861019</td>\n",
       "      <td>18.826037</td>\n",
       "      <td>18.885899</td>\n",
       "      <td>(feat_315, feat_336, feat_639, feat_867, feat_...</td>\n",
       "      <td>18.857651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>18.946047</td>\n",
       "      <td>18.931851</td>\n",
       "      <td>18.957128</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_867, feat_...</td>\n",
       "      <td>18.945009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14774</th>\n",
       "      <td>18.921960</td>\n",
       "      <td>18.930747</td>\n",
       "      <td>19.035931</td>\n",
       "      <td>(feat_526, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>18.962879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12049</th>\n",
       "      <td>18.906654</td>\n",
       "      <td>18.963113</td>\n",
       "      <td>19.055451</td>\n",
       "      <td>(feat_336, feat_526, feat_639, feat_867, feat_...</td>\n",
       "      <td>18.975073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12288</th>\n",
       "      <td>19.004289</td>\n",
       "      <td>19.031871</td>\n",
       "      <td>19.010707</td>\n",
       "      <td>(feat_336, feat_639, feat_829, feat_867, feat_...</td>\n",
       "      <td>19.015622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>19.231228</td>\n",
       "      <td>19.272457</td>\n",
       "      <td>19.269392</td>\n",
       "      <td>(feat_336, feat_341, feat_639, feat_829, feat_...</td>\n",
       "      <td>19.257692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12233</th>\n",
       "      <td>19.275495</td>\n",
       "      <td>19.272129</td>\n",
       "      <td>19.319550</td>\n",
       "      <td>(feat_336, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>19.289058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>19.303867</td>\n",
       "      <td>19.304873</td>\n",
       "      <td>19.297138</td>\n",
       "      <td>(feat_257, feat_336, feat_639, feat_829, feat_...</td>\n",
       "      <td>19.301960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>19.552526</td>\n",
       "      <td>19.540212</td>\n",
       "      <td>19.537948</td>\n",
       "      <td>(feat_257, feat_336, feat_639, feat_867, feat_...</td>\n",
       "      <td>19.543562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>19.695547</td>\n",
       "      <td>19.656106</td>\n",
       "      <td>19.720021</td>\n",
       "      <td>(feat_315, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>19.690558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15504 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Xdb_1_corr_sum  Xdb_2_corr_sum  Xdb_3_corr_sum  \\\n",
       "3445         5.875845        5.872196        5.888319   \n",
       "3729         7.691497        7.655846        7.592507   \n",
       "14895        7.637696        7.655231        7.667335   \n",
       "15322        7.832736        7.839899        7.783426   \n",
       "3199         7.837366        7.894866        7.896389   \n",
       "14888        7.899120        7.893090        7.891945   \n",
       "3718         7.948210        7.918848        7.854657   \n",
       "3349         7.923472        7.955926        7.884594   \n",
       "2308         7.954591        7.909024        7.965330   \n",
       "1242         8.039368        7.934679        7.966066   \n",
       "3565         8.049228        8.019662        7.940713   \n",
       "1293         8.070623        7.973879        7.979041   \n",
       "3280         8.082042        8.074152        7.995866   \n",
       "12070        8.041623        8.053898        8.090013   \n",
       "8485         8.108501        8.141844        8.032311   \n",
       "2958         8.114682        8.103782        8.081794   \n",
       "9226         8.129768        8.088761        8.097298   \n",
       "2774         8.146899        8.108159        8.077372   \n",
       "14070        8.210892        8.236887        8.140580   \n",
       "8751         8.239205        8.167829        8.206500   \n",
       "3060         8.252071        8.253272        8.139390   \n",
       "3514         8.217414        8.268188        8.278337   \n",
       "14731        8.241858        8.273622        8.268059   \n",
       "1264         8.348064        8.254235        8.192323   \n",
       "14891        8.292666        8.331288        8.303295   \n",
       "15358        8.326981        8.333203        8.281753   \n",
       "2672         8.316888        8.282006        8.344493   \n",
       "14864        8.297316        8.324891        8.335630   \n",
       "2410         8.413909        8.330627        8.272169   \n",
       "7262         8.381765        8.347559        8.369122   \n",
       "...               ...             ...             ...   \n",
       "5622        18.167595       18.230791       18.201692   \n",
       "9713        18.204316       18.175931       18.228693   \n",
       "1905        18.222520       18.217978       18.232539   \n",
       "10748       18.235253       18.207738       18.321367   \n",
       "10684       18.239700       18.219710       18.340381   \n",
       "3663        18.306723       18.320649       18.305784   \n",
       "11882       18.316137       18.343520       18.328314   \n",
       "1555        18.332220       18.308996       18.349334   \n",
       "4965        18.441188       18.370424       18.414225   \n",
       "12883       18.401539       18.425342       18.413806   \n",
       "11289       18.461992       18.533534       18.607556   \n",
       "12595       18.516303       18.539757       18.561910   \n",
       "5057        18.587545       18.515597       18.520000   \n",
       "4967        18.599210       18.538693       18.587681   \n",
       "1998        18.696703       18.708055       18.701262   \n",
       "11378       18.665386       18.695446       18.753856   \n",
       "10657       18.747550       18.706990       18.819577   \n",
       "3608        18.758017       18.765334       18.772074   \n",
       "5058        18.815842       18.759403       18.781236   \n",
       "1907        18.837957       18.843159       18.877827   \n",
       "9649        18.861019       18.826037       18.885899   \n",
       "1934        18.946047       18.931851       18.957128   \n",
       "14774       18.921960       18.930747       19.035931   \n",
       "12049       18.906654       18.963113       19.055451   \n",
       "12288       19.004289       19.031871       19.010707   \n",
       "11376       19.231228       19.272457       19.269392   \n",
       "12233       19.275495       19.272129       19.319550   \n",
       "2387        19.303867       19.304873       19.297138   \n",
       "2389        19.552526       19.540212       19.537948   \n",
       "10868       19.695547       19.656106       19.720021   \n",
       "\n",
       "                                                 columns  mean_corr_sum  \n",
       "3445   (feat_257, feat_526, feat_681, feat_736, feat_...       5.878787  \n",
       "3729   (feat_257, feat_681, feat_736, feat_920, feat_...       7.646617  \n",
       "14895  (feat_526, feat_681, feat_736, feat_920, feat_...       7.653421  \n",
       "15322  (feat_681, feat_724, feat_736, feat_920, feat_...       7.818687  \n",
       "3199   (feat_257, feat_504, feat_526, feat_736, feat_...       7.876207  \n",
       "14888  (feat_526, feat_681, feat_736, feat_808, feat_...       7.894719  \n",
       "3718   (feat_257, feat_681, feat_736, feat_769, feat_...       7.907238  \n",
       "3349   (feat_257, feat_504, feat_736, feat_769, feat_...       7.921331  \n",
       "2308   (feat_257, feat_336, feat_526, feat_681, feat_...       7.942982  \n",
       "1242   (feat_257, feat_308, feat_504, feat_701, feat_...       7.980038  \n",
       "3565   (feat_257, feat_639, feat_681, feat_736, feat_...       8.003201  \n",
       "1293   (feat_257, feat_308, feat_526, feat_681, feat_...       8.007848  \n",
       "3280   (feat_257, feat_504, feat_681, feat_736, feat_...       8.050687  \n",
       "12070  (feat_336, feat_526, feat_681, feat_736, feat_...       8.061845  \n",
       "8485   (feat_308, feat_395, feat_681, feat_701, feat_...       8.094219  \n",
       "2958   (feat_257, feat_395, feat_526, feat_681, feat_...       8.100086  \n",
       "9226   (feat_308, feat_701, feat_769, feat_808, feat_...       8.105276  \n",
       "2774   (feat_257, feat_341, feat_681, feat_736, feat_...       8.110810  \n",
       "14070  (feat_395, feat_681, feat_736, feat_920, feat_...       8.196120  \n",
       "8751   (feat_308, feat_504, feat_701, feat_769, feat_...       8.204512  \n",
       "3060   (feat_257, feat_395, feat_681, feat_736, feat_...       8.214911  \n",
       "3514   (feat_257, feat_526, feat_736, feat_769, feat_...       8.254646  \n",
       "14731  (feat_526, feat_639, feat_681, feat_736, feat_...       8.261180  \n",
       "1264   (feat_257, feat_308, feat_504, feat_769, feat_...       8.264874  \n",
       "14891  (feat_526, feat_681, feat_736, feat_829, feat_...       8.309083  \n",
       "15358  (feat_681, feat_736, feat_808, feat_920, feat_...       8.313979  \n",
       "2672   (feat_257, feat_341, feat_526, feat_681, feat_...       8.314463  \n",
       "14864  (feat_526, feat_681, feat_724, feat_736, feat_...       8.319279  \n",
       "2410   (feat_257, feat_336, feat_681, feat_736, feat_...       8.338902  \n",
       "7262   (feat_308, feat_315, feat_504, feat_769, feat_...       8.366148  \n",
       "...                                                  ...            ...  \n",
       "5622   (feat_269, feat_341, feat_395, feat_724, feat_...      18.200026  \n",
       "9713   (feat_315, feat_336, feat_701, feat_867, feat_...      18.202980  \n",
       "1905   (feat_257, feat_315, feat_639, feat_701, feat_...      18.224345  \n",
       "10748  (feat_315, feat_526, feat_701, feat_867, feat_...      18.254786  \n",
       "10684  (feat_315, feat_526, feat_639, feat_867, feat_...      18.266597  \n",
       "3663   (feat_257, feat_639, feat_829, feat_867, feat_...      18.311052  \n",
       "11882  (feat_336, feat_504, feat_639, feat_829, feat_...      18.329324  \n",
       "1555   (feat_257, feat_315, feat_336, feat_639, feat_...      18.330184  \n",
       "4965   (feat_269, feat_315, feat_639, feat_701, feat_...      18.408612  \n",
       "12883  (feat_341, feat_504, feat_639, feat_829, feat_...      18.413563  \n",
       "11289  (feat_336, feat_341, feat_526, feat_639, feat_...      18.534361  \n",
       "12595  (feat_341, feat_395, feat_526, feat_724, feat_...      18.539323  \n",
       "5057   (feat_269, feat_315, feat_701, feat_867, feat_...      18.541048  \n",
       "4967   (feat_269, feat_315, feat_639, feat_701, feat_...      18.575194  \n",
       "1998   (feat_257, feat_315, feat_701, feat_867, feat_...      18.702007  \n",
       "11378  (feat_336, feat_341, feat_639, feat_867, feat_...      18.704896  \n",
       "10657  (feat_315, feat_526, feat_639, feat_701, feat_...      18.758039  \n",
       "3608   (feat_257, feat_639, feat_701, feat_867, feat_...      18.765142  \n",
       "5058   (feat_269, feat_315, feat_701, feat_867, feat_...      18.785494  \n",
       "1907   (feat_257, feat_315, feat_639, feat_701, feat_...      18.852981  \n",
       "9649   (feat_315, feat_336, feat_639, feat_867, feat_...      18.857651  \n",
       "1934   (feat_257, feat_315, feat_639, feat_867, feat_...      18.945009  \n",
       "14774  (feat_526, feat_639, feat_701, feat_867, feat_...      18.962879  \n",
       "12049  (feat_336, feat_526, feat_639, feat_867, feat_...      18.975073  \n",
       "12288  (feat_336, feat_639, feat_829, feat_867, feat_...      19.015622  \n",
       "11376  (feat_336, feat_341, feat_639, feat_829, feat_...      19.257692  \n",
       "12233  (feat_336, feat_639, feat_701, feat_867, feat_...      19.289058  \n",
       "2387   (feat_257, feat_336, feat_639, feat_829, feat_...      19.301960  \n",
       "2389   (feat_257, feat_336, feat_639, feat_867, feat_...      19.543562  \n",
       "10868  (feat_315, feat_639, feat_701, feat_867, feat_...      19.690558  \n",
       "\n",
       "[15504 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_results_df = pd.DataFrame(corr_results)\n",
    "corr_results_df.sort_values('mean_corr_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_results_df.loc[3445, 'columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive testing of the 5 identified features. Since we suspect that these 5 features may be the 'true' predictors, we will exclude and feature selection and/or dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier(weights='distance'))])\n",
    "\n",
    "rfc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_all_pipes_reduced(X, y):\n",
    "    X_reduced = X[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size = 0.25, random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for pipe in tqdm_notebook([dtc_pipe, lr_pipe, knn_pipe, rfc_pipe, svc_pipe]):\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = pipe.score(X_train, y_train)\n",
    "        test_score = pipe.score(X_test, y_test)\n",
    "        \n",
    "        scores.append({'classifier': pipe.named_steps['classifier'],\n",
    "                      'train_score': train_score,\n",
    "                      'test_score': test_score})\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a584ff13847241e5a78bfcdebda29356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b4606a2d841f4b32b9545e06480b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8b6bc5866d40bc9ee2954234c07420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xdb_1_reduced_naive = test_all_pipes_reduced(Xdb_1, ydb_1)\n",
    "Xdb_2_reduced_naive = test_all_pipes_reduced(Xdb_2, ydb_2)\n",
    "Xdb_3_reduced_naive = test_all_pipes_reduced(Xdb_3, ydb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.743937</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.601051</td>\n",
       "      <td>0.610389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.826395</td>\n",
       "      <td>0.886613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.796686</td>\n",
       "      <td>0.989692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.793438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.743937     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.601051     0.610389\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.826395     0.886613\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.796686     0.989692\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.787793     0.793438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_1_reduced_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.740304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.617553</td>\n",
       "      <td>0.599374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.883298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.789684</td>\n",
       "      <td>0.991469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.778289</td>\n",
       "      <td>0.788256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.740304     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.617553     0.599374\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.827469     0.883298\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.789684     0.991469\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.778289     0.788256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_2_reduced_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.736158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.597242</td>\n",
       "      <td>0.611381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.825305</td>\n",
       "      <td>0.885587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.787128</td>\n",
       "      <td>0.989871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.769938</td>\n",
       "      <td>0.784367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.736158     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.597242     0.611381\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.825305     0.885587\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.787128     0.989871\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.769938     0.784367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_3_reduced_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These naive test scores are looking nearly as strong as my tuned models utilizing PCA, suggesting that I may have identified the true predictors. Let's GridSearch some parameters and see what kind of scores we can achieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdb_true1 = Xdb_1[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "Xdb_true2 = Xdb_2[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "Xdb_true3 = Xdb_3[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier(weights='distance'))])\n",
    "\n",
    "rfc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])\n",
    "\n",
    "dtc_params = {'classifier__max_depth': [1, 3, 5, 10, 15, None],\n",
    "                  'classifier__splitter': ['random', 'best']}\n",
    "\n",
    "lr_params = {'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__max_iter': [100, 500],\n",
    "                 'classifier__C': np.logspace(-3,3,7)}\n",
    "\n",
    "knn_params = {'classifier__algorithm': ['auto'],\n",
    "                  'classifier__p': [2, 3],\n",
    "                  'classifier__n_neighbors': np.linspace(1,10).astype(int)}\n",
    "\n",
    "rfc_params = {'classifier__n_estimators': [10, 50, 100, 200, 500],\n",
    "                  'classifier__max_features': ['log2', 'sqrt', 'auto'],\n",
    "                  'classifier__max_depth': [1, 5, None]}\n",
    "\n",
    "svc_params = {'classifier__C': np.logspace(-3,3,15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_gs(X_train, y_train, X_test, y_test, pipe, param):\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param, cv=5, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best params:', gs.best_params_)\n",
    "#     print('Best fitting score:', gs.best_score_)\n",
    "#     print('Train score:', gs.score(X_train, y_train))\n",
    "    print('Test score:', gs.score(X_test, y_test))\n",
    "    \n",
    "    return gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xdb_true1, ydb_1, test_size = 0.25, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(Xdb_true2, ydb_2, test_size = 0.25, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(Xdb_true3, ydb_3, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdb_true1\n",
      "Decision Tree\n",
      "Best params: {'classifier__max_depth': 10, 'classifier__splitter': 'best'}\n",
      "Test score: 0.743734842361\n",
      "\n",
      "LogReg\n",
      "Best params: {'classifier__C': 0.01, 'classifier__max_iter': 100, 'classifier__penalty': 'l1'}\n",
      "Test score: 0.599029911075\n",
      "\n",
      "Random Forest\n",
      "Best params: {'classifier__max_depth': None, 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 500}\n",
      "Test score: 0.822958771221\n",
      "\n",
      "KNN\n",
      "Best params: {'classifier__algorithm': 'auto', 'classifier__n_neighbors': 8, 'classifier__p': 2}\n",
      "Test score: 0.837712206952\n",
      "\n",
      "SVC\n",
      "Best params: {'classifier__C': 1000.0}\n",
      "Test score: 0.825586095392\n",
      "Xdb_true2\n",
      "Decision Tree\n",
      "Best params: {'classifier__max_depth': 10, 'classifier__splitter': 'best'}\n",
      "Test score: 0.74450219912\n",
      "\n",
      "LogReg\n",
      "Best params: {'classifier__C': 1.0, 'classifier__max_iter': 100, 'classifier__penalty': 'l1'}\n",
      "Test score: 0.617952818872\n",
      "\n",
      "Random Forest\n",
      "Best params: {'classifier__max_depth': None, 'classifier__max_features': 'log2', 'classifier__n_estimators': 500}\n",
      "Test score: 0.821071571371\n",
      "\n",
      "KNN\n",
      "Best params: {'classifier__algorithm': 'auto', 'classifier__n_neighbors': 10, 'classifier__p': 2}\n",
      "Test score: 0.830467812875\n",
      "\n",
      "SVC\n",
      "Best params: {'classifier__C': 1000.0}\n",
      "Test score: 0.814074370252\n",
      "Xdb_true3\n",
      "Decision Tree\n",
      "Best params: {'classifier__max_depth': 10, 'classifier__splitter': 'best'}\n",
      "Test score: 0.736957825305\n",
      "\n",
      "LogReg\n",
      "Best params: {'classifier__C': 0.01, 'classifier__max_iter': 100, 'classifier__penalty': 'l1'}\n",
      "Test score: 0.600239856086\n",
      "\n",
      "Random Forest\n",
      "Best params: {'classifier__max_depth': None, 'classifier__max_features': 'log2', 'classifier__n_estimators': 200}\n",
      "Test score: 0.821906855886\n",
      "\n",
      "KNN\n",
      "Best params: {'classifier__algorithm': 'auto', 'classifier__n_neighbors': 10, 'classifier__p': 2}\n",
      "Test score: 0.832100739556\n",
      "\n",
      "SVC\n",
      "Best params: {'classifier__C': 1000.0}\n",
      "Test score: 0.816909854088\n"
     ]
    }
   ],
   "source": [
    "print(\"Xdb_true1\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, svc_pipe, svc_params)\n",
    "\n",
    "print(\"Xdb_true2\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, svc_pipe, svc_params)\n",
    "\n",
    "print(\"Xdb_true3\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, svc_pipe, svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are on par with my best tuned models using PCA, further indicating that I have isolated the 5 true features. Let's see if the models can be further tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "rfc_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])\n",
    "\n",
    "mnb_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', MinMaxScaler(feature_range=(1,2))),\n",
    "                     ('classifier', MultinomialNB())])\n",
    "\n",
    "gnb_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', GaussianNB())])\n",
    "\n",
    "knn_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__algorithm': ['auto', 'ball_tree'],\n",
    "              'classifier__weights': ['distance'],\n",
    "              'classifier__n_neighbors': np.linspace(5,15).astype(int)}\n",
    "\n",
    "rfc_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__n_estimators': [ 500],\n",
    "\n",
    "              'classifier__max_depth': [None]}\n",
    "\n",
    "svc_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__C': np.logspace(0,4,15)\n",
    "             }\n",
    "\n",
    "mnb_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__alpha': np.linspace(0,2,8)\n",
    "             }\n",
    "\n",
    "gnb_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__priors': [(0.5,0.5)]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   1.93069773e+00,   3.72759372e+00,\n",
       "         7.19685673e+00,   1.38949549e+01,   2.68269580e+01,\n",
       "         5.17947468e+01,   1.00000000e+02,   1.93069773e+02,\n",
       "         3.72759372e+02,   7.19685673e+02,   1.38949549e+03,\n",
       "         2.68269580e+03,   5.17947468e+03,   1.00000000e+04])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0,4,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdb_true1\n",
      "\n",
      "MNB\n",
      "Best params: {'classifier__alpha': 0.0, 'poly__degree': 5, 'poly__include_bias': True, 'poly__interaction_only': False}\n",
      "Test score: 0.577607113985\n",
      "\n",
      "GNB\n",
      "Best params: {'classifier__priors': (0.5, 0.5), 'poly__degree': 2, 'poly__include_bias': True, 'poly__interaction_only': True}\n",
      "Test score: 0.639854486661\n",
      "\n",
      "Random Forest\n",
      "Best params: {'classifier__max_depth': None, 'classifier__n_estimators': 500, 'poly__degree': 2, 'poly__include_bias': True, 'poly__interaction_only': False}\n",
      "Test score: 0.823565076799\n",
      "\n",
      "KNN\n",
      "Best params: {'classifier__algorithm': 'auto', 'classifier__n_neighbors': 8, 'classifier__weights': 'distance', 'poly__degree': 2, 'poly__include_bias': True, 'poly__interaction_only': False}\n",
      "Test score: 0.829830234438\n",
      "\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "print(\"Xdb_true1\")\n",
    "print(\"\\nMNB\")\n",
    "mnb_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, mnb_pipe, mnb_params)\n",
    "print(\"\\nGNB\")\n",
    "gnb_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, gnb_pipe, gnb_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, svc_pipe, svc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('poly', PolynomialFeatures(5)),\n",
    "                     ('classifier', MultinomialNB())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('poly', PolynomialFeatures(degree=5, include_bias=True, interaction_only=False)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__alpha': array([ 0.     ,  0.28571,  0.57143,  0.85714,  1.14286,  1.42857,\n",
       "        1.71429,  2.     ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_gs = GridSearchCV(mnb_pipe, mnb_params, cv=5, n_jobs = -1)\n",
    "mnb_gs.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60610347615198057"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_gs.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Naive Bayes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "lr_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__penalty': ['l1', 'l2'],\n",
    "             'classifier__max_iter': [100, 500],\n",
    "             'classifier__C': np.logspace(-3,3,7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78213419563459985"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs = GridSearchCV(lr_pipe, lr_params, cv=5, n_jobs = -1)\n",
    "lr_gs.fit(X1_train, y1_train)\n",
    "lr_gs.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 100.0,\n",
       " 'classifier__max_iter': 100,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'poly__degree': 5,\n",
       " 'poly__include_bias': False,\n",
       " 'poly__interaction_only': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe test a voting classifier of 5 LogRegs, each individual classifier using a separate feature to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, maybe test `from sklearn.neural_network import MLPClassifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_257 = LogisticRegression(C = 100, max_iter = 100, penalty = 'l1')\n",
    "lr_526 = LogisticRegression(C = 100, max_iter = 100, penalty = 'l1')\n",
    "lr_681 = LogisticRegression(C = 100, max_iter = 100, penalty = 'l1')\n",
    "lr_736 = LogisticRegression(C = 100, max_iter = 100, penalty = 'l1')\n",
    "lr_920 = LogisticRegression(C = 100, max_iter = 100, penalty = 'l1')\n",
    "\n",
    "lr_257.fit(X1_train['feat_257'], y1_train)\n",
    "lr_526.fit(X1_train['feat_526'], y1_train)\n",
    "lr_681.fit(X1_train['feat_681'], y1_train)\n",
    "lr_736.fit(X1_train['feat_736'], y1_train)\n",
    "lr_920.fit(X1_train['feat_920'], y1_train)\n",
    "\n",
    "lr_ensemble_df = pd.DataFrame({'lr_257': lr_257.predict(X1_test['feat_257']),\n",
    "                               'lr_526': lr_526.predict(X1_test['feat_526']),\n",
    "                               'lr_681': lr_681.predict(X1_test['feat_681']),\n",
    "                               'lr_736': lr_736.predict(X1_test['feat_736']),\n",
    "                               'lr_920': lr_920.predict(X1_test['feat_920']),\n",
    "                              'class': y1_train})\n",
    "\n",
    "['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
