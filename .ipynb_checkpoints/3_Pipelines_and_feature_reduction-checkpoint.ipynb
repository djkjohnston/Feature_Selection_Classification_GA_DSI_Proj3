{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%run data_package_loading.py # Code loads data as well as packages that are relevant across most project phases\n",
    "%matplotlib inline\n",
    "\n",
    "uci_features = ['28',  '48',  '64', '105', '128', '153', '241', '281', '318', '336', \n",
    "                '338', '378', '433', '442', '451', '453', '455', '472', '475', '493']\n",
    "\n",
    "madelon_features = ['feat_257', 'feat_269', 'feat_308', 'feat_315', 'feat_336',\n",
    "                   'feat_341', 'feat_395', 'feat_504', 'feat_526', 'feat_639',\n",
    "                   'feat_681', 'feat_701', 'feat_724', 'feat_736', 'feat_769',\n",
    "                   'feat_808', 'feat_829', 'feat_867', 'feat_920', 'feat_956']\n",
    "\n",
    "Xuci_1 = Xuci_1[uci_features]\n",
    "Xuci_2 = Xuci_2[uci_features]\n",
    "Xuci_3 = Xuci_3[uci_features]\n",
    "\n",
    "# !conda install -y psycopg2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel, RFECV \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "\n",
    "Xdb_1 = pd.read_pickle('data/madelon_db_1')\n",
    "Xdb_2 = pd.read_pickle('data/madelon_db_2')\n",
    "Xdb_3 = pd.read_pickle('data/madelon_db_3')\n",
    "\n",
    "\n",
    "ydb_1 = Xdb_1['target']\n",
    "ydb_2 = Xdb_2['target']\n",
    "ydb_3 = Xdb_3['target']\n",
    "Xdb_1 = Xdb_1[madelon_features]\n",
    "Xdb_2 = Xdb_2[madelon_features]\n",
    "Xdb_3 = Xdb_3[madelon_features]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boxcox import BoxCoxTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force of testing Feature Correlations. \n",
    "\n",
    "This approach is based on two pieces of information:\n",
    "1. Previous grid searches of PCA consistently found that 5 PCA components provided the best fit. This suggests that the Database data includes five true predictors.\n",
    "2. the true predictors are independent from each other.\n",
    "\n",
    "The following code will attempt to find the set of 5 feature with the least amount of correlations. This will be accomplished by \n",
    "1. Testing every set of 5 features from the 20 features previously identified.\n",
    "1. Creating a cross correlation matrix for each set of 5\n",
    "1. Taking the sum of the absolute values of the correlation matrix.\n",
    "\n",
    "The assumption is that the set of 5 features with the lowest total correlation will be the most independent set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbd0058ff2a475b91e0d7eb7eab39ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15504), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corr_results = []\n",
    "combos = list(itertools.combinations(Xdb_1, 5))\n",
    "\n",
    "for cols in tqdm_notebook(combos):\n",
    "    corr1 = Xdb_1[list(cols)].corr()\n",
    "    corr2 = Xdb_2[list(cols)].corr()\n",
    "    corr3 = Xdb_3[list(cols)].corr()\n",
    "\n",
    "    # zero out the diagonal\n",
    "    for corr_df in [corr1, corr2, corr3]:\n",
    "        for i in corr_df.columns:\n",
    "            corr_df.loc[i,i] = 0\n",
    "\n",
    "    tmp = pd.concat([corr1, corr2, corr3])\n",
    "    mean_corr = abs(tmp).groupby(tmp.index).mean() #removes diagnals\n",
    "\n",
    "\n",
    "    corr_results.append({'columns': cols,\n",
    "                         'Xdb_1_corr_sum': abs(corr1).sum().sum(),\n",
    "                         'Xdb_2_corr_sum': abs(corr2).sum().sum(),\n",
    "                         'Xdb_3_corr_sum': abs(corr3).sum().sum(),\n",
    "                         'mean_corr_sum': mean_corr.sum().sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xdb_1_corr_sum</th>\n",
       "      <th>Xdb_2_corr_sum</th>\n",
       "      <th>Xdb_3_corr_sum</th>\n",
       "      <th>columns</th>\n",
       "      <th>mean_corr_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>0.875845</td>\n",
       "      <td>0.872196</td>\n",
       "      <td>0.888319</td>\n",
       "      <td>(feat_257, feat_526, feat_681, feat_736, feat_...</td>\n",
       "      <td>0.878787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>2.691497</td>\n",
       "      <td>2.655846</td>\n",
       "      <td>2.592507</td>\n",
       "      <td>(feat_257, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>2.646617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14895</th>\n",
       "      <td>2.637696</td>\n",
       "      <td>2.655231</td>\n",
       "      <td>2.667335</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>2.653421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15322</th>\n",
       "      <td>2.832736</td>\n",
       "      <td>2.839899</td>\n",
       "      <td>2.783426</td>\n",
       "      <td>(feat_681, feat_724, feat_736, feat_920, feat_...</td>\n",
       "      <td>2.818687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2.837366</td>\n",
       "      <td>2.894866</td>\n",
       "      <td>2.896389</td>\n",
       "      <td>(feat_257, feat_504, feat_526, feat_736, feat_...</td>\n",
       "      <td>2.876207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14888</th>\n",
       "      <td>2.899120</td>\n",
       "      <td>2.893090</td>\n",
       "      <td>2.891945</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_808, feat_...</td>\n",
       "      <td>2.894719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718</th>\n",
       "      <td>2.948210</td>\n",
       "      <td>2.918848</td>\n",
       "      <td>2.854657</td>\n",
       "      <td>(feat_257, feat_681, feat_736, feat_769, feat_...</td>\n",
       "      <td>2.907238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>2.923472</td>\n",
       "      <td>2.955926</td>\n",
       "      <td>2.884594</td>\n",
       "      <td>(feat_257, feat_504, feat_736, feat_769, feat_...</td>\n",
       "      <td>2.921331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2.954591</td>\n",
       "      <td>2.909024</td>\n",
       "      <td>2.965330</td>\n",
       "      <td>(feat_257, feat_336, feat_526, feat_681, feat_...</td>\n",
       "      <td>2.942982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>3.039368</td>\n",
       "      <td>2.934679</td>\n",
       "      <td>2.966066</td>\n",
       "      <td>(feat_257, feat_308, feat_504, feat_701, feat_...</td>\n",
       "      <td>2.980038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>3.049228</td>\n",
       "      <td>3.019662</td>\n",
       "      <td>2.940713</td>\n",
       "      <td>(feat_257, feat_639, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.003201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>3.070623</td>\n",
       "      <td>2.973879</td>\n",
       "      <td>2.979041</td>\n",
       "      <td>(feat_257, feat_308, feat_526, feat_681, feat_...</td>\n",
       "      <td>3.007848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>3.082042</td>\n",
       "      <td>3.074152</td>\n",
       "      <td>2.995866</td>\n",
       "      <td>(feat_257, feat_504, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.050687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12070</th>\n",
       "      <td>3.041623</td>\n",
       "      <td>3.053898</td>\n",
       "      <td>3.090013</td>\n",
       "      <td>(feat_336, feat_526, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.061845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>3.108501</td>\n",
       "      <td>3.141844</td>\n",
       "      <td>3.032311</td>\n",
       "      <td>(feat_308, feat_395, feat_681, feat_701, feat_...</td>\n",
       "      <td>3.094219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>3.114682</td>\n",
       "      <td>3.103782</td>\n",
       "      <td>3.081794</td>\n",
       "      <td>(feat_257, feat_395, feat_526, feat_681, feat_...</td>\n",
       "      <td>3.100086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9226</th>\n",
       "      <td>3.129768</td>\n",
       "      <td>3.088761</td>\n",
       "      <td>3.097298</td>\n",
       "      <td>(feat_308, feat_701, feat_769, feat_808, feat_...</td>\n",
       "      <td>3.105276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>3.146899</td>\n",
       "      <td>3.108159</td>\n",
       "      <td>3.077372</td>\n",
       "      <td>(feat_257, feat_341, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.110810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070</th>\n",
       "      <td>3.210892</td>\n",
       "      <td>3.236887</td>\n",
       "      <td>3.140580</td>\n",
       "      <td>(feat_395, feat_681, feat_736, feat_920, feat_...</td>\n",
       "      <td>3.196120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>3.239205</td>\n",
       "      <td>3.167829</td>\n",
       "      <td>3.206500</td>\n",
       "      <td>(feat_308, feat_504, feat_701, feat_769, feat_...</td>\n",
       "      <td>3.204512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>3.252071</td>\n",
       "      <td>3.253272</td>\n",
       "      <td>3.139390</td>\n",
       "      <td>(feat_257, feat_395, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.214911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>3.217414</td>\n",
       "      <td>3.268188</td>\n",
       "      <td>3.278337</td>\n",
       "      <td>(feat_257, feat_526, feat_736, feat_769, feat_...</td>\n",
       "      <td>3.254646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>3.241858</td>\n",
       "      <td>3.273622</td>\n",
       "      <td>3.268059</td>\n",
       "      <td>(feat_526, feat_639, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.261180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>3.348064</td>\n",
       "      <td>3.254235</td>\n",
       "      <td>3.192323</td>\n",
       "      <td>(feat_257, feat_308, feat_504, feat_769, feat_...</td>\n",
       "      <td>3.264874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14891</th>\n",
       "      <td>3.292666</td>\n",
       "      <td>3.331288</td>\n",
       "      <td>3.303295</td>\n",
       "      <td>(feat_526, feat_681, feat_736, feat_829, feat_...</td>\n",
       "      <td>3.309083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15358</th>\n",
       "      <td>3.326981</td>\n",
       "      <td>3.333203</td>\n",
       "      <td>3.281753</td>\n",
       "      <td>(feat_681, feat_736, feat_808, feat_920, feat_...</td>\n",
       "      <td>3.313979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>3.316888</td>\n",
       "      <td>3.282006</td>\n",
       "      <td>3.344493</td>\n",
       "      <td>(feat_257, feat_341, feat_526, feat_681, feat_...</td>\n",
       "      <td>3.314463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14864</th>\n",
       "      <td>3.297316</td>\n",
       "      <td>3.324891</td>\n",
       "      <td>3.335630</td>\n",
       "      <td>(feat_526, feat_681, feat_724, feat_736, feat_...</td>\n",
       "      <td>3.319279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>3.413909</td>\n",
       "      <td>3.330627</td>\n",
       "      <td>3.272169</td>\n",
       "      <td>(feat_257, feat_336, feat_681, feat_736, feat_...</td>\n",
       "      <td>3.338902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7262</th>\n",
       "      <td>3.381765</td>\n",
       "      <td>3.347559</td>\n",
       "      <td>3.369122</td>\n",
       "      <td>(feat_308, feat_315, feat_504, feat_769, feat_...</td>\n",
       "      <td>3.366148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>13.167595</td>\n",
       "      <td>13.230791</td>\n",
       "      <td>13.201692</td>\n",
       "      <td>(feat_269, feat_341, feat_395, feat_724, feat_...</td>\n",
       "      <td>13.200026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>13.204316</td>\n",
       "      <td>13.175931</td>\n",
       "      <td>13.228693</td>\n",
       "      <td>(feat_315, feat_336, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.202980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>13.222520</td>\n",
       "      <td>13.217978</td>\n",
       "      <td>13.232539</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>13.224345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>13.235253</td>\n",
       "      <td>13.207738</td>\n",
       "      <td>13.321367</td>\n",
       "      <td>(feat_315, feat_526, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.254786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10684</th>\n",
       "      <td>13.239700</td>\n",
       "      <td>13.219710</td>\n",
       "      <td>13.340381</td>\n",
       "      <td>(feat_315, feat_526, feat_639, feat_867, feat_...</td>\n",
       "      <td>13.266597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>13.306723</td>\n",
       "      <td>13.320649</td>\n",
       "      <td>13.305784</td>\n",
       "      <td>(feat_257, feat_639, feat_829, feat_867, feat_...</td>\n",
       "      <td>13.311052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11882</th>\n",
       "      <td>13.316137</td>\n",
       "      <td>13.343520</td>\n",
       "      <td>13.328314</td>\n",
       "      <td>(feat_336, feat_504, feat_639, feat_829, feat_...</td>\n",
       "      <td>13.329324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>13.332220</td>\n",
       "      <td>13.308996</td>\n",
       "      <td>13.349334</td>\n",
       "      <td>(feat_257, feat_315, feat_336, feat_639, feat_...</td>\n",
       "      <td>13.330184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>13.441188</td>\n",
       "      <td>13.370424</td>\n",
       "      <td>13.414225</td>\n",
       "      <td>(feat_269, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>13.408612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12883</th>\n",
       "      <td>13.401539</td>\n",
       "      <td>13.425342</td>\n",
       "      <td>13.413806</td>\n",
       "      <td>(feat_341, feat_504, feat_639, feat_829, feat_...</td>\n",
       "      <td>13.413563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>13.461992</td>\n",
       "      <td>13.533534</td>\n",
       "      <td>13.607556</td>\n",
       "      <td>(feat_336, feat_341, feat_526, feat_639, feat_...</td>\n",
       "      <td>13.534361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>13.516303</td>\n",
       "      <td>13.539757</td>\n",
       "      <td>13.561910</td>\n",
       "      <td>(feat_341, feat_395, feat_526, feat_724, feat_...</td>\n",
       "      <td>13.539323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>13.587545</td>\n",
       "      <td>13.515597</td>\n",
       "      <td>13.520000</td>\n",
       "      <td>(feat_269, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.541048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>13.599210</td>\n",
       "      <td>13.538693</td>\n",
       "      <td>13.587681</td>\n",
       "      <td>(feat_269, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>13.575194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>13.696703</td>\n",
       "      <td>13.708055</td>\n",
       "      <td>13.701262</td>\n",
       "      <td>(feat_257, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.702007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11378</th>\n",
       "      <td>13.665386</td>\n",
       "      <td>13.695446</td>\n",
       "      <td>13.753856</td>\n",
       "      <td>(feat_336, feat_341, feat_639, feat_867, feat_...</td>\n",
       "      <td>13.704896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>13.747550</td>\n",
       "      <td>13.706990</td>\n",
       "      <td>13.819577</td>\n",
       "      <td>(feat_315, feat_526, feat_639, feat_701, feat_...</td>\n",
       "      <td>13.758039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>13.758017</td>\n",
       "      <td>13.765334</td>\n",
       "      <td>13.772074</td>\n",
       "      <td>(feat_257, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.765142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>13.815842</td>\n",
       "      <td>13.759403</td>\n",
       "      <td>13.781236</td>\n",
       "      <td>(feat_269, feat_315, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.785494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>13.837957</td>\n",
       "      <td>13.843159</td>\n",
       "      <td>13.877827</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_701, feat_...</td>\n",
       "      <td>13.852981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>13.861019</td>\n",
       "      <td>13.826037</td>\n",
       "      <td>13.885899</td>\n",
       "      <td>(feat_315, feat_336, feat_639, feat_867, feat_...</td>\n",
       "      <td>13.857651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>13.946047</td>\n",
       "      <td>13.931851</td>\n",
       "      <td>13.957128</td>\n",
       "      <td>(feat_257, feat_315, feat_639, feat_867, feat_...</td>\n",
       "      <td>13.945009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14774</th>\n",
       "      <td>13.921960</td>\n",
       "      <td>13.930747</td>\n",
       "      <td>14.035931</td>\n",
       "      <td>(feat_526, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>13.962879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12049</th>\n",
       "      <td>13.906654</td>\n",
       "      <td>13.963113</td>\n",
       "      <td>14.055451</td>\n",
       "      <td>(feat_336, feat_526, feat_639, feat_867, feat_...</td>\n",
       "      <td>13.975073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12288</th>\n",
       "      <td>14.004289</td>\n",
       "      <td>14.031871</td>\n",
       "      <td>14.010707</td>\n",
       "      <td>(feat_336, feat_639, feat_829, feat_867, feat_...</td>\n",
       "      <td>14.015622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>14.231228</td>\n",
       "      <td>14.272457</td>\n",
       "      <td>14.269392</td>\n",
       "      <td>(feat_336, feat_341, feat_639, feat_829, feat_...</td>\n",
       "      <td>14.257692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12233</th>\n",
       "      <td>14.275495</td>\n",
       "      <td>14.272129</td>\n",
       "      <td>14.319550</td>\n",
       "      <td>(feat_336, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>14.289058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>14.303867</td>\n",
       "      <td>14.304873</td>\n",
       "      <td>14.297138</td>\n",
       "      <td>(feat_257, feat_336, feat_639, feat_829, feat_...</td>\n",
       "      <td>14.301960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>14.552526</td>\n",
       "      <td>14.540212</td>\n",
       "      <td>14.537948</td>\n",
       "      <td>(feat_257, feat_336, feat_639, feat_867, feat_...</td>\n",
       "      <td>14.543562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>14.695547</td>\n",
       "      <td>14.656106</td>\n",
       "      <td>14.720021</td>\n",
       "      <td>(feat_315, feat_639, feat_701, feat_867, feat_...</td>\n",
       "      <td>14.690558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15504 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Xdb_1_corr_sum  Xdb_2_corr_sum  Xdb_3_corr_sum  \\\n",
       "3445         0.875845        0.872196        0.888319   \n",
       "3729         2.691497        2.655846        2.592507   \n",
       "14895        2.637696        2.655231        2.667335   \n",
       "15322        2.832736        2.839899        2.783426   \n",
       "3199         2.837366        2.894866        2.896389   \n",
       "14888        2.899120        2.893090        2.891945   \n",
       "3718         2.948210        2.918848        2.854657   \n",
       "3349         2.923472        2.955926        2.884594   \n",
       "2308         2.954591        2.909024        2.965330   \n",
       "1242         3.039368        2.934679        2.966066   \n",
       "3565         3.049228        3.019662        2.940713   \n",
       "1293         3.070623        2.973879        2.979041   \n",
       "3280         3.082042        3.074152        2.995866   \n",
       "12070        3.041623        3.053898        3.090013   \n",
       "8485         3.108501        3.141844        3.032311   \n",
       "2958         3.114682        3.103782        3.081794   \n",
       "9226         3.129768        3.088761        3.097298   \n",
       "2774         3.146899        3.108159        3.077372   \n",
       "14070        3.210892        3.236887        3.140580   \n",
       "8751         3.239205        3.167829        3.206500   \n",
       "3060         3.252071        3.253272        3.139390   \n",
       "3514         3.217414        3.268188        3.278337   \n",
       "14731        3.241858        3.273622        3.268059   \n",
       "1264         3.348064        3.254235        3.192323   \n",
       "14891        3.292666        3.331288        3.303295   \n",
       "15358        3.326981        3.333203        3.281753   \n",
       "2672         3.316888        3.282006        3.344493   \n",
       "14864        3.297316        3.324891        3.335630   \n",
       "2410         3.413909        3.330627        3.272169   \n",
       "7262         3.381765        3.347559        3.369122   \n",
       "...               ...             ...             ...   \n",
       "5622        13.167595       13.230791       13.201692   \n",
       "9713        13.204316       13.175931       13.228693   \n",
       "1905        13.222520       13.217978       13.232539   \n",
       "10748       13.235253       13.207738       13.321367   \n",
       "10684       13.239700       13.219710       13.340381   \n",
       "3663        13.306723       13.320649       13.305784   \n",
       "11882       13.316137       13.343520       13.328314   \n",
       "1555        13.332220       13.308996       13.349334   \n",
       "4965        13.441188       13.370424       13.414225   \n",
       "12883       13.401539       13.425342       13.413806   \n",
       "11289       13.461992       13.533534       13.607556   \n",
       "12595       13.516303       13.539757       13.561910   \n",
       "5057        13.587545       13.515597       13.520000   \n",
       "4967        13.599210       13.538693       13.587681   \n",
       "1998        13.696703       13.708055       13.701262   \n",
       "11378       13.665386       13.695446       13.753856   \n",
       "10657       13.747550       13.706990       13.819577   \n",
       "3608        13.758017       13.765334       13.772074   \n",
       "5058        13.815842       13.759403       13.781236   \n",
       "1907        13.837957       13.843159       13.877827   \n",
       "9649        13.861019       13.826037       13.885899   \n",
       "1934        13.946047       13.931851       13.957128   \n",
       "14774       13.921960       13.930747       14.035931   \n",
       "12049       13.906654       13.963113       14.055451   \n",
       "12288       14.004289       14.031871       14.010707   \n",
       "11376       14.231228       14.272457       14.269392   \n",
       "12233       14.275495       14.272129       14.319550   \n",
       "2387        14.303867       14.304873       14.297138   \n",
       "2389        14.552526       14.540212       14.537948   \n",
       "10868       14.695547       14.656106       14.720021   \n",
       "\n",
       "                                                 columns  mean_corr_sum  \n",
       "3445   (feat_257, feat_526, feat_681, feat_736, feat_...       0.878787  \n",
       "3729   (feat_257, feat_681, feat_736, feat_920, feat_...       2.646617  \n",
       "14895  (feat_526, feat_681, feat_736, feat_920, feat_...       2.653421  \n",
       "15322  (feat_681, feat_724, feat_736, feat_920, feat_...       2.818687  \n",
       "3199   (feat_257, feat_504, feat_526, feat_736, feat_...       2.876207  \n",
       "14888  (feat_526, feat_681, feat_736, feat_808, feat_...       2.894719  \n",
       "3718   (feat_257, feat_681, feat_736, feat_769, feat_...       2.907238  \n",
       "3349   (feat_257, feat_504, feat_736, feat_769, feat_...       2.921331  \n",
       "2308   (feat_257, feat_336, feat_526, feat_681, feat_...       2.942982  \n",
       "1242   (feat_257, feat_308, feat_504, feat_701, feat_...       2.980038  \n",
       "3565   (feat_257, feat_639, feat_681, feat_736, feat_...       3.003201  \n",
       "1293   (feat_257, feat_308, feat_526, feat_681, feat_...       3.007848  \n",
       "3280   (feat_257, feat_504, feat_681, feat_736, feat_...       3.050687  \n",
       "12070  (feat_336, feat_526, feat_681, feat_736, feat_...       3.061845  \n",
       "8485   (feat_308, feat_395, feat_681, feat_701, feat_...       3.094219  \n",
       "2958   (feat_257, feat_395, feat_526, feat_681, feat_...       3.100086  \n",
       "9226   (feat_308, feat_701, feat_769, feat_808, feat_...       3.105276  \n",
       "2774   (feat_257, feat_341, feat_681, feat_736, feat_...       3.110810  \n",
       "14070  (feat_395, feat_681, feat_736, feat_920, feat_...       3.196120  \n",
       "8751   (feat_308, feat_504, feat_701, feat_769, feat_...       3.204512  \n",
       "3060   (feat_257, feat_395, feat_681, feat_736, feat_...       3.214911  \n",
       "3514   (feat_257, feat_526, feat_736, feat_769, feat_...       3.254646  \n",
       "14731  (feat_526, feat_639, feat_681, feat_736, feat_...       3.261180  \n",
       "1264   (feat_257, feat_308, feat_504, feat_769, feat_...       3.264874  \n",
       "14891  (feat_526, feat_681, feat_736, feat_829, feat_...       3.309083  \n",
       "15358  (feat_681, feat_736, feat_808, feat_920, feat_...       3.313979  \n",
       "2672   (feat_257, feat_341, feat_526, feat_681, feat_...       3.314463  \n",
       "14864  (feat_526, feat_681, feat_724, feat_736, feat_...       3.319279  \n",
       "2410   (feat_257, feat_336, feat_681, feat_736, feat_...       3.338902  \n",
       "7262   (feat_308, feat_315, feat_504, feat_769, feat_...       3.366148  \n",
       "...                                                  ...            ...  \n",
       "5622   (feat_269, feat_341, feat_395, feat_724, feat_...      13.200026  \n",
       "9713   (feat_315, feat_336, feat_701, feat_867, feat_...      13.202980  \n",
       "1905   (feat_257, feat_315, feat_639, feat_701, feat_...      13.224345  \n",
       "10748  (feat_315, feat_526, feat_701, feat_867, feat_...      13.254786  \n",
       "10684  (feat_315, feat_526, feat_639, feat_867, feat_...      13.266597  \n",
       "3663   (feat_257, feat_639, feat_829, feat_867, feat_...      13.311052  \n",
       "11882  (feat_336, feat_504, feat_639, feat_829, feat_...      13.329324  \n",
       "1555   (feat_257, feat_315, feat_336, feat_639, feat_...      13.330184  \n",
       "4965   (feat_269, feat_315, feat_639, feat_701, feat_...      13.408612  \n",
       "12883  (feat_341, feat_504, feat_639, feat_829, feat_...      13.413563  \n",
       "11289  (feat_336, feat_341, feat_526, feat_639, feat_...      13.534361  \n",
       "12595  (feat_341, feat_395, feat_526, feat_724, feat_...      13.539323  \n",
       "5057   (feat_269, feat_315, feat_701, feat_867, feat_...      13.541048  \n",
       "4967   (feat_269, feat_315, feat_639, feat_701, feat_...      13.575194  \n",
       "1998   (feat_257, feat_315, feat_701, feat_867, feat_...      13.702007  \n",
       "11378  (feat_336, feat_341, feat_639, feat_867, feat_...      13.704896  \n",
       "10657  (feat_315, feat_526, feat_639, feat_701, feat_...      13.758039  \n",
       "3608   (feat_257, feat_639, feat_701, feat_867, feat_...      13.765142  \n",
       "5058   (feat_269, feat_315, feat_701, feat_867, feat_...      13.785494  \n",
       "1907   (feat_257, feat_315, feat_639, feat_701, feat_...      13.852981  \n",
       "9649   (feat_315, feat_336, feat_639, feat_867, feat_...      13.857651  \n",
       "1934   (feat_257, feat_315, feat_639, feat_867, feat_...      13.945009  \n",
       "14774  (feat_526, feat_639, feat_701, feat_867, feat_...      13.962879  \n",
       "12049  (feat_336, feat_526, feat_639, feat_867, feat_...      13.975073  \n",
       "12288  (feat_336, feat_639, feat_829, feat_867, feat_...      14.015622  \n",
       "11376  (feat_336, feat_341, feat_639, feat_829, feat_...      14.257692  \n",
       "12233  (feat_336, feat_639, feat_701, feat_867, feat_...      14.289058  \n",
       "2387   (feat_257, feat_336, feat_639, feat_829, feat_...      14.301960  \n",
       "2389   (feat_257, feat_336, feat_639, feat_867, feat_...      14.543562  \n",
       "10868  (feat_315, feat_639, feat_701, feat_867, feat_...      14.690558  \n",
       "\n",
       "[15504 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_results_df = pd.DataFrame(corr_results)\n",
    "corr_results_df.sort_values('mean_corr_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_results_df.loc[3445, 'columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive testing of the 5 identified features. Since we suspect that these 5 features may be the 'true' predictors, we will exclude and feature selection and/or dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier(weights='distance'))])\n",
    "\n",
    "rfc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_all_pipes_reduced(X, y):\n",
    "    X_reduced = X[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size = 0.25, random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for pipe in tqdm_notebook([dtc_pipe, lr_pipe, knn_pipe, rfc_pipe, svc_pipe]):\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = pipe.score(X_train, y_train)\n",
    "        test_score = pipe.score(X_test, y_test)\n",
    "        \n",
    "        scores.append({'classifier': pipe.named_steps['classifier'],\n",
    "                      'train_score': train_score,\n",
    "                      'test_score': test_score})\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a584ff13847241e5a78bfcdebda29356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b4606a2d841f4b32b9545e06480b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8b6bc5866d40bc9ee2954234c07420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xdb_1_reduced_naive = test_all_pipes_reduced(Xdb_1, ydb_1)\n",
    "Xdb_2_reduced_naive = test_all_pipes_reduced(Xdb_2, ydb_2)\n",
    "Xdb_3_reduced_naive = test_all_pipes_reduced(Xdb_3, ydb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.743937</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.601051</td>\n",
       "      <td>0.610389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.826395</td>\n",
       "      <td>0.886613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.796686</td>\n",
       "      <td>0.989692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.793438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.743937     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.601051     0.610389\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.826395     0.886613\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.796686     0.989692\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.787793     0.793438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_1_reduced_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.740304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.617553</td>\n",
       "      <td>0.599374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.883298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.789684</td>\n",
       "      <td>0.991469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.778289</td>\n",
       "      <td>0.788256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.740304     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.617553     0.599374\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.827469     0.883298\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.789684     0.991469\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.778289     0.788256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_2_reduced_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.736158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.597242</td>\n",
       "      <td>0.611381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.825305</td>\n",
       "      <td>0.885587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.787128</td>\n",
       "      <td>0.989871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.769938</td>\n",
       "      <td>0.784367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          classifier  test_score  train_score\n",
       "0  DecisionTreeClassifier(class_weight=None, crit...    0.736158     1.000000\n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...    0.597242     0.611381\n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...    0.825305     0.885587\n",
       "3  (DecisionTreeClassifier(class_weight=None, cri...    0.787128     0.989871\n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...    0.769938     0.784367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdb_3_reduced_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These naive test scores are looking nearly as strong as my tuned models utilizing PCA, suggesting that I may have identified the true predictors. Let's GridSearch some parameters and see what kind of scores we can achieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdb_true1 = Xdb_1[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "Xdb_true2 = Xdb_2[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]\n",
    "Xdb_true3 = Xdb_3[['feat_257', 'feat_526', 'feat_681', 'feat_736', 'feat_920']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier(weights='distance'))])\n",
    "\n",
    "rfc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])\n",
    "\n",
    "dtc_params = {'classifier__max_depth': [1, 3, 5, 10, 15, None],\n",
    "                  'classifier__splitter': ['random', 'best']}\n",
    "\n",
    "lr_params = {'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__max_iter': [100, 500],\n",
    "                 'classifier__C': np.logspace(-3,3,7)}\n",
    "\n",
    "knn_params = {'classifier__algorithm': ['auto'],\n",
    "                  'classifier__p': [2, 3],\n",
    "                  'classifier__n_neighbors': np.linspace(1,10).astype(int)}\n",
    "\n",
    "rfc_params = {'classifier__n_estimators': [10, 50, 100, 200, 500],\n",
    "                  'classifier__max_features': ['log2', 'sqrt', 'auto'],\n",
    "                  'classifier__max_depth': [1, 5, None]}\n",
    "\n",
    "svc_params = {'classifier__C': np.logspace(-3,3,15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_gs(X_train, y_train, X_test, y_test, pipe, param):\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param, cv=5, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best params:', gs.best_params_)\n",
    "#     print('Best fitting score:', gs.best_score_)\n",
    "#     print('Train score:', gs.score(X_train, y_train))\n",
    "    print('Test score:', gs.score(X_test, y_test))\n",
    "    \n",
    "    return gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xdb_true1, ydb_1, test_size = 0.25, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(Xdb_true2, ydb_2, test_size = 0.25, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(Xdb_true3, ydb_3, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdb_true1\n",
      "Decision Tree\n",
      "Best params: {'classifier__max_depth': 10, 'classifier__splitter': 'best'}\n",
      "Test score: 0.744139046079\n",
      "\n",
      "LogReg\n",
      "Best params: {'classifier__C': 0.01, 'classifier__max_iter': 100, 'classifier__penalty': 'l1'}\n",
      "Test score: 0.599029911075\n",
      "\n",
      "Random Forest\n",
      "Best params: {'classifier__max_depth': None, 'classifier__max_features': 'log2', 'classifier__n_estimators': 500}\n",
      "Test score: 0.824373484236\n",
      "\n",
      "KNN\n",
      "Best params: {'classifier__algorithm': 'auto', 'classifier__n_neighbors': 8, 'classifier__p': 2}\n",
      "Test score: 0.836903799515\n",
      "\n",
      "SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-304:\n",
      "Process ForkPoolWorker-301:\n",
      "Process ForkPoolWorker-303:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-298:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-297:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "print(\"Xdb_true1\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, svc_pipe, svc_params)\n",
    "\n",
    "print(\"Xdb_true2\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X2_train, y2_train, X2_test, y2_test, svc_pipe, svc_params)\n",
    "\n",
    "print(\"Xdb_true3\")\n",
    "print(\"Decision Tree\")\n",
    "dtc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, dtc_pipe, dtc_params)\n",
    "print(\"\\nLogReg\")\n",
    "lr_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, lr_pipe, lr_params)\n",
    "print(\"\\nRandom Forest\")\n",
    "rfc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, rfc_pipe, rfc_params)\n",
    "print(\"\\nKNN\")\n",
    "knn_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_pca_classifier = test_train_gs(X3_train, y3_train, X3_test, y3_test, svc_pipe, svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are on par with my best tuned models using PCA, further indicating that I have isolated the 5 true features. Let's see if the models can be further tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "rfc_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "svc_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', SVC(probability=True))])\n",
    "\n",
    "mnb_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', MinMaxScaler(feature_range=(1,2))),\n",
    "                     ('classifier', MultinomialNB())])\n",
    "\n",
    "gnb_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('classifier', GaussianNB())])\n",
    "\n",
    "knn_params = {'poly__degree': [1, 2, 3],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__algorithm': ['auto', 'ball_tree'],\n",
    "              'classifier__weights': ['distance'],\n",
    "              'classifier__n_neighbors': np.linspace(5,15).astype(int)}\n",
    "\n",
    "rfc_params = {'poly__degree': [1, 2, 3],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__n_estimators': [ 500],\n",
    "\n",
    "              'classifier__max_depth': [None]}\n",
    "\n",
    "svc_params = {'poly__degree': [1, 2, 3],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__C': np.logspace(0,4,15)\n",
    "             }\n",
    "\n",
    "mnb_params = {'poly__degree': [1, 2, 3],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__alpha': np.linspace(0,2,8)\n",
    "             }\n",
    "\n",
    "gnb_params = {'poly__degree': [1, 2, 3],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__priors': [(0.5,0.5)]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   1.93069773e+00,   3.72759372e+00,\n",
       "         7.19685673e+00,   1.38949549e+01,   2.68269580e+01,\n",
       "         5.17947468e+01,   1.00000000e+02,   1.93069773e+02,\n",
       "         3.72759372e+02,   7.19685673e+02,   1.38949549e+03,\n",
       "         2.68269580e+03,   5.17947468e+03,   1.00000000e+04])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0,4,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7676327446c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# knn_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, knn_pipe, knn_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSVC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msvc_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_train_gs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fca2e62b76e0>\u001b[0m in \u001b[0;36mtest_train_gs\u001b[0;34m(X_train, y_train, X_test, y_test, pipe, param)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best params:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"Xdb_true1\")\n",
    "# print(\"\\nMNB\")\n",
    "# mnb_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, mnb_pipe, mnb_params)\n",
    "# print(\"\\nGNB\")\n",
    "# gnb_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, gnb_pipe, gnb_params)\n",
    "# print(\"\\nRandom Forest\")\n",
    "# rfc_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, rfc_pipe, rfc_params)\n",
    "# print(\"\\nKNN\")\n",
    "# knn_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, knn_pipe, knn_params)\n",
    "print(\"\\nSVC\")\n",
    "svc_classifier = test_train_gs(X1_train, y1_train, X1_test, y1_test, svc_pipe, svc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('poly', PolynomialFeatures(5)),\n",
    "                     ('classifier', MultinomialNB())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('poly', PolynomialFeatures(degree=5, include_bias=True, interaction_only=False)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__alpha': array([ 0.     ,  0.28571,  0.57143,  0.85714,  1.14286,  1.42857,\n",
       "        1.71429,  2.     ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_gs = GridSearchCV(mnb_pipe, mnb_params, cv=5, n_jobs = -1)\n",
    "mnb_gs.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60610347615198057"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_gs.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Naive Bayes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())])\n",
    "\n",
    "lr_params = {'poly__degree': [2, 3, 4, 5],\n",
    "             'poly__interaction_only': [True, False],\n",
    "             'poly__include_bias': [True, False],\n",
    "             'classifier__penalty': ['l1', 'l2'],\n",
    "             'classifier__max_iter': [100, 500],\n",
    "             'classifier__C': np.logspace(-3,3,7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78213419563459985"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs = GridSearchCV(lr_pipe, lr_params, cv=5, n_jobs = -1)\n",
    "lr_gs.fit(X1_train, y1_train)\n",
    "lr_gs.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 100.0,\n",
       " 'classifier__max_iter': 100,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'poly__degree': 5,\n",
       " 'poly__include_bias': False,\n",
       " 'poly__interaction_only': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe test a voting classifier of 5 LogRegs, each individual classifier using a separate feature to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, maybe test `from sklearn.neural_network import MLPClassifier`\n",
    "X1_train, X1_test, y1_train, y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', MLPClassifier())])\n",
    "\n",
    "mlp_params = {\n",
    "              'classifier__alpha': np.logspace(-3,3,7),\n",
    "              'classifier__learning_rate_init': np.logspace(-4,0,5)}\n",
    "\n",
    "# 'classifier__hidden_layer_sizes': [(10,), (50,), (100,), (200,)],\n",
    "#               'classifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "#               'classifier__solver': ['lbfgs', 'sgd', 'adam'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
